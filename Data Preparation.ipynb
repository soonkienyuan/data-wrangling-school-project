{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7379495",
   "metadata": {},
   "source": [
    "# Data Preparation \n",
    "\n",
    "Show and explain all steps involved in your assignment â€“ Data Import, Data Cleaning, Data Preview, Data Description. Must have flow chart.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390b849",
   "metadata": {},
   "source": [
    "## First view of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b87fc",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b042e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "data_file_path = \"\"\n",
    "csv_file_name = \"Austin Bicycle Crashes 2010-2017.csv\"\n",
    "dataset = pd.read_csv(data_file_path+csv_file_name)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce820da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.matrix(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3950e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfc0df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9cf548",
   "metadata": {},
   "source": [
    "### Store and Restore the Fields Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fc314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the fields in another csv file\n",
    "data_file_path = \"\"\n",
    "csv_file_name = \"Austin Bicycle Crashes 2010-2017.csv\"\n",
    "\n",
    "def save_df_csv(csv_str, file_name, path=\"\"):\n",
    "\tf = open(path+file_name, \"w\")\n",
    "\tf.write(csv_str)\n",
    "\tf.close()\n",
    "li = list(dataset.columns)\n",
    "df = pd.DataFrame(li, columns=[\"fields\"])\n",
    "save_df_csv(df.to_csv(index=False, line_terminator=\"\\n\"), \"fields before grouping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d656e5be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = \"data fields grouping.csv\"\n",
    "super_grp = pd.read_csv(file_path)\n",
    "super_grp = {grp_name: super_grp[grp_name].dropna() for grp_name in super_grp}\n",
    "# check\n",
    "from functools import reduce\n",
    "grp_set = reduce(lambda acm, cur: acm.union(cur), [super_grp[n] for n in super_grp], set())\n",
    "assert(set([n for n in dataset]).difference(grp_set) == set())\n",
    "pprint(super_grp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12aec10",
   "metadata": {},
   "source": [
    "### Example Using Super Grouping (stub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53024c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_super_grp_print(): \n",
    "\t[i for i in super_grp]\n",
    "\tdataset.loc[:, list(super_grp[\"crash_report\"])].head()\n",
    "\tdataset.loc[:, list(super_grp[\"geo_info\"])].head()\n",
    "\tdataset.loc[:, list(super_grp[\"day\"])]\n",
    "\tdataset.loc[:, list(super_grp[\"person\"])]\n",
    "\tdataset.loc[:, list(super_grp[\"road_cond\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5301a074",
   "metadata": {},
   "source": [
    "### Checking Data\n",
    "Note that: it is easier to view the data superficially using Excel\n",
    "\n",
    "The data are revised by going through each 10 of all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni abbrv. one\n",
    "uni_col = list(filter(lambda c: len(dataset[c].unique()) == 1, dataset))\n",
    "uni_dict = {k:dataset[k].unique()[0] for k in uni_col}\n",
    "uni_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the column that has only one unique data\n",
    "dataset.drop([col for col in uni_dict],axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.replace([\"No Data\"], pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd8486",
   "metadata": {},
   "source": [
    "Remark: There is a field `street number` use `\"NO DATA\"` and might be up to a interpretation that where `street number` is no applicable to every street. Hence, we keep it as it is, then evaluated later on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba298fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see there are how many variables\n",
    "len(list(dataset.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac19e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leg_stepping(start, end,step = 1):\n",
    "\t'''\n",
    "\t\teg.: it = leg_stepping(1, 10, 3)\n",
    "\t\tprint(next(it)) -> (1, 4)\n",
    "\t\tprint(next(it)) -> (5, 8)\n",
    "\t\tprint(next(it)) -> (9, 10)\n",
    "\t'''\n",
    "\tassert(start < end)\n",
    "\twhile start+step < end:\n",
    "\t\tyield {\"start\": start, \"end\": start+step}\n",
    "\t\tstart += step+1\n",
    "\tyield {\"start\": start, \"end\": end}\n",
    "ind_iter = leg_stepping(0, len(list(dataset.dtypes)), 10)\n",
    "tmp_viewer = lambda curr: dataset.iloc[:, curr[\"start\"]:curr[\"end\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_slice = next(ind_iter)\n",
    "tmp_viewer(curr_slice).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa34f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [\"$1000 Damage to Any One Person's Property\", \"Active School Zone Flag\", \"Construction Zone Flag\"]\n",
    "{c:dataset[c].unique() for c in li} # check if binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2d874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def booleanize(s, true_corrpd, false_corrpd):\n",
    "\ts = s.replace(true_corrpd, True)\n",
    "\ts = s.replace(false_corrpd, False)\n",
    "\treturn s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the yes/no to boolean values\n",
    "li = [\"$1000 Damage to Any One Person's Property\", \"Active School Zone Flag\", \"Construction Zone Flag\"]\n",
    "dataset.loc[:,li] = booleanize(dataset.loc[:,li], \"Yes\", \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_slice = next(ind_iter)\n",
    "tmp_viewer(curr_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "converter = lambda n: datetime.time(hour = int(n/100), minute = n%100)\n",
    "dataset['Crash Time'] = dataset['Crash Time'].apply(converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5d270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_viewer(curr_slice).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c3bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_slice = next(ind_iter)\n",
    "tmp_viewer(curr_slice).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa18361",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_viewer(curr_slice).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d210b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Intersecting Street Name\"].replace(np.NAN, pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb4a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_slice = next(ind_iter)\n",
    "tmp_viewer(curr_slice).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Property Damages\"].unique()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Property Damages\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [\"Private Drive Flag\"]\n",
    "dataset.loc[:,li] = booleanize(dataset.loc[:,li], \"Yes\", \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83911f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_slice = next(ind_iter)\n",
    "tmp_viewer(curr_slice).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b9654",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_viewer(curr_slice).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ff115",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ['Surface Condition','Traffic Control Type','Weather Condition','Person Helmet']\n",
    "{c:dataset[c].unique() for c in s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ff47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010db240",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74200077",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_file_name = lambda f: f.split(\".\")[0]\n",
    "save_df_csv(dataset.to_csv(index=False, line_terminator=\"\\n\"),\n",
    "file_name=f\"{extract_file_name(csv_file_name)} revised.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2807f74",
   "metadata": {},
   "source": [
    "## Cleaning and Pre-process of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e5812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "data_file_path = \"\"\n",
    "csv_file_name = \"Austin Bicycle Crashes 2010-2017 revised.csv\"\n",
    "dataset = pd.read_csv(data_file_path+csv_file_name)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9dd6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop_duplicates('Crash ID', inplace=True) # assuming Crash ID is the primary key\n",
    "# Visualising missing value\n",
    "import missingno as msno\n",
    "msno.matrix(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1257152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another visualization of missing values based on counting\n",
    "dataset.count().plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e46fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from previous boxplot, we decide drop the columns whose counts less then 2000\n",
    "li = filter(lambda col_nm: dataset[col_nm].count() < 2000, dataset)\n",
    "li = list(li)\n",
    "li # the list of columns will be drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa550c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(li, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab43e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.count(1)[0] # number of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8678361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.matrix(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e239a7b",
   "metadata": {},
   "source": [
    "### Dropping Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8df76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may assume that Crash ID is the primary key\n",
    "# hence drop Case ID\n",
    "dataset.drop(\"Case ID\", axis=1, inplace = True) \n",
    "dataset = dataset.dropna()\n",
    "msno.matrix(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52d405",
   "metadata": {},
   "source": [
    "We have reach at the point where there is no obvious missing data, thus the data is cleaner than initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2415bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Proj, transform\n",
    "# cartography(map making field), all maps are projected\n",
    "# then we use Mercator projection here\n",
    "lons, lats = [], []\n",
    "inProj = Proj(init='epsg:3857')\n",
    "outProj = Proj(init='epsg:4326')\n",
    "for lon, lat in list(zip(dataset[\"Longitude\"], dataset[\"Latitude\"])):\n",
    "    x, y = transform(outProj,inProj,lon,lat)\n",
    "    lons.append(x)\n",
    "    lats.append(y)\n",
    "dataset[\"MercatorX\"] = lons\n",
    "dataset[\"MercatorY\"] = lats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7eb70",
   "metadata": {},
   "source": [
    "### Saving file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da4b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_csv(csv_str, file_name, path=\"\"):\n",
    "\tf = open(path+file_name, \"w\")\n",
    "\tf.write(csv_str)\n",
    "\tf.close()\n",
    "extract_file_name = lambda f: f.split(\".\")[0]\n",
    "save_df_csv(dataset.to_csv(index=False, line_terminator=\"\\n\"),\n",
    "file_name=f\"{extract_file_name(csv_file_name)} subset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796fd239",
   "metadata": {},
   "source": [
    "## To discover outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c06f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pprint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e90d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/taimoon/data-wrangling-school-project/main/Austin%20Bicycle%20Crashes%202010-2017%20revised%20subset.csv\"\n",
    "df=pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list  for column indexing\n",
    "outlier_col=[\"Crash Death Count\",\"Crash Incapacitating Injury Count\",\"Crash Non-incapacitating Injury Count\",\n",
    "             \"Crash Not Injured Count\",\"Crash Possible Injury Count\",\"Crash Total Injury Count\",\n",
    "             \"Crash Unknown Injury Count\",\"Speed Limit\" ]\n",
    "print(len(outlier_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd18db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dict to store location of outlier\n",
    "out_dict = {}\n",
    "new_key=0\n",
    "for b in outlier_col:\n",
    "    z = np.abs(stats.zscore(df[b]))\n",
    "    out_index= np.where(z>3)\n",
    "    \n",
    "    # add into dictionary\n",
    "    new_key+=1\n",
    "    out_dict[b] = out_index\n",
    "\n",
    "\n",
    "pprint.pprint(out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to extract outlier value\n",
    "\n",
    "for b in outlier_col:\n",
    "    print(\"Outlier value for \" + b)\n",
    "    print(df[b].iloc[out_dict[b]])\n",
    "    print(\" \")\n",
    "    print(\"-------------------------------------------------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e343735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the latest version of Pandas there is an easy way to do exactly this. \n",
    "#Column names (which are strings) can be sliced in whatever manner you like\n",
    "df_measures= pd.DataFrame(df, columns=outlier_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36cb997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box Plot\n",
    "for col in outlier_col:\n",
    "    sns.boxplot( y=df[col] )\n",
    "    plt.figure()   # plots figure for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram\n",
    "for col in outlier_col:\n",
    "    sns.histplot(data=df[col])\n",
    "    plt.figure()   # plots figure for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = \"Speed Limit\"\n",
    "y_columns = [\"Crash Death Count\",\"Crash Incapacitating Injury Count\",\"Crash Non-incapacitating Injury Count\",\n",
    "             \"Crash Not Injured Count\",\"Crash Possible Injury Count\",\"Crash Total Injury Count\",\n",
    "             \"Crash Unknown Injury Count\"]\n",
    "\n",
    "outlier_col=[\"Crash Death Count\",\"Crash Incapacitating Injury Count\",\"Crash Non-incapacitating Injury Count\",\n",
    "             \"Crash Not Injured Count\",\"Crash Possible Injury Count\",\"Crash Total Injury Count\",\n",
    "             \"Crash Unknown Injury Count\",\"Speed Limit\" ]\n",
    "for y_col in y_columns:\n",
    "\n",
    "    figure = plt.figure\n",
    "    ax = plt.gca()\n",
    "    ax.scatter(df[x_col], df[y_col])\n",
    "    #ax.set_xlabel(x_col)\n",
    "    #ax.set_ylabel(y_col)\n",
    "    ax.set_title(\"{} vs {}\".format(x_col, y_col))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21893455",
   "metadata": {},
   "source": [
    "## Relationship Between Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2cc521",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/taimoon/data-wrangling-school-project/main/Austin%20Bicycle%20Crashes%202010-2017%20revised%20subset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abb91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let we present by heat map\n",
    "\n",
    "sb.heatmap(df.corr(),cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55ddc3f",
   "metadata": {},
   "source": [
    "In the above particular heatmap with this color map, the dark color means that the correlation is very high. Our diagonal is of course very dark and then we can see that **between crash total injury count and Crash Incapacitating Injury Count** got also dark blue cube, which means that they are very correlative.\n",
    "\n",
    "We also can see that between MercatorX and longitude are very correlative. It can also be seen that MercatorY has very high correlations with latitude. The combination between \"logitude and latitude\" and \"MercatorY and MercatorX\" are correlated together too.\n",
    "We can explained that MercatorX, longitude, latitude and MercatorY are correlated each other is because they are geographic  measurement that related to each other, and often used and appear in combination. For example, (15Â°24'15\"N, 30Â°10'3\"E) for latitude and longitude. The MercatorY and MercatorX can be converted into latitude and longitude respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87396989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let we just select particular row\n",
    "list_selected=[\"Crash Total Injury Count\", \"Crash Unknown Injury Count\", \"Speed Limit\"]\n",
    "df_selected=pd.DataFrame(df, columns=list_selected)\n",
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.heatmap(df_selected.corr(),cmap=\"RdBu_r\", annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe53f5a",
   "metadata": {},
   "source": [
    "To zoom in more details, it is clearly show there are no correlation between Crash Total ijury Count, Crash unknown injury count and speed limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d357b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(df_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe523c03",
   "metadata": {},
   "source": [
    "Now, we will run a pairplot, which takes every two variables and shows us their scatter versus each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c495aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let create pairplot for crash total injury count and Crash Incapacitating Injury Count.\n",
    "\n",
    "sb.pairplot(df,\n",
    "    x_vars=\"Crash Total Injury Count\",\n",
    "    y_vars=\"Crash Incapacitating Injury Count\",\n",
    "    diag_kws={'bins':30}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc71aa1",
   "metadata": {},
   "source": [
    "When we go deep into the correlation between rash total injury count and Crash Incapacitating Injury Count, we cannot make statement that they have correlation. This is because they creates the plot without the regression line."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
